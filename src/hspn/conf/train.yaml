defaults:
  - base
  - _self_

# Basic training options -------------------------------------------------------
seed: 42
n_epochs: 2
comm_backend: nccl
log_interval: 50 # number of batches
checkpoint_dir: ${hydra:runtime.output_dir}

# Model config -----------------------------------------------------------------
model:
  _target_: hspn.model.DeepOperatorNet
  branch_dim: 1
  trunk_dim: 3
  branch_config:
    width: 100
    depth: 4
    activation:
      _target_: torch.nn.ELU
  trunk_config:
    width: 100
    depth: 5
    activation:
      _target_: torch.nn.ReLU
  latent_dim: 25
  einsum_pattern: ij,kj->ik

# Data configuration
dataloader: &train_loader
  _target_: torch.utils.data.DataLoader
  dataset: &train_dataset
    _target_: hspn.dataset.H5Dataset
    file_path: ${extra.data_dir}/don_dataset.h5
    branch_batch_size: null
    trunk_batch_size: 100_000
    start: 0.0
    end: 0.8
  batch_size: null  # Do not change this when using `hspn.dataset.H5Dataloader` which already yields batches
  shuffle: false # Do not change this when using `hspn.dataset.H5Dataloader` which is an `torch.utils.data.IterableDataset``
  num_workers: 0
  pin_memory: true
  prefetch_factor: null
val_dataloader:
  <<: *train_loader
  dataset:
    <<: *train_dataset
    start: 0.8
    end: 1.0

# Optimizer configuration
optimizer_factory:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0005
  # lr: ${optimizer.lr}
  # weight_decay: ${optimizer.weight_decay}

# Scheduler configuration
scheduler_factory: null
#   _target_: torch.optim.lr_scheduler.${scheduler.name}
#   _partial_: true
#   T_max: ${n_epochs}
#   eta_min: ${scheduler.eta_min}

tracker_config: ${extra.aim_config}

extra:
  data_dir: ${oc.env:DATA_DIR,./data}
  tb_config:
    log_dir: ${checkpoint_dir}
    backend: tensorboard
    experiment_name: ${now:%Y-%m-%d_%H-%M-%S}_${hydra:job.name}
  aim_config:
    log_dir: .
    backend: aim
    experiment_name: ${hydra:job.name}

