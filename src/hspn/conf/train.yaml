#
# HyperSPIN code - hspn_surrogate_models
#
# Copyright 2025 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Licensed under a MIT (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software each subject to its own license.
#
# DM25-0396
#

defaults:
  - base
  - _self_

# Basic training options -------------------------------------------------------
seed: 42
n_epochs: 2
comm_backend: nccl
log_interval: 100 # number of batches
checkpoint_dir: ${hydra:runtime.output_dir}
enable_amp: false
grad_accum_steps: 1
enable_grad_scaling: false
grad_clip_norm: null

# Model config -----------------------------------------------------------------
model:
  _target_: hspn.model.DeepOperatorNet
  branch_dim: 1
  trunk_dim: 3
  branch_config:
    width: 100
    depth: 4
    activation:
      _target_: torch.nn.ELU
  trunk_config:
    width: 100
    depth: 5
    activation:
      _target_: torch.nn.ReLU
  latent_dim: 25
  einsum_pattern: ij,kj->ik

# Data config ------------------------------------------------------------------
dataloader: &train_loader
  _target_: torch.utils.data.DataLoader
  dataset: &train_dataset
    _target_: hspn.dataset.H5Dataset
    file_path: ${oc.env:DATA_DIR,./data}/don_dataset.h5
    branch_batch_size: null
    trunk_batch_size: 100_000
    branch_start: 0.0 # integer offset or float 0.0 <= x <= 1.0 for a percentage
    branch_end: 0.8
  batch_size: null # Do not change this when using `hspn.dataset.H5Dataset` which is a `torch.utils.data.IterableDataset` and already yields batches
  shuffle: false # Do not change this when using `hspn.dataset.H5Dataset` which is a `torch.utils.data.IterableDataset` and yields batches in a deterministic pattern
  num_workers: 0
  pin_memory: true
  prefetch_factor: null
val_dataloader:
  <<: *train_loader
  dataset:
    <<: *train_dataset
    branch_start: 0.8
    branch_end: 1.0

# Optimizer config -------------------------------------------------------------
optimizer:
  _target_: torch.optim.Adam
  lr: 0.0001
  # lr: ${optimizer.lr}
  # weight_decay: ${optimizer.weight_decay}

# Scheduler config (optional) --------------------------------------------------
scheduler: null # set to null to disable
# scheduler:
#  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#  T_max: ${n_epochs}
#
# scheduler:
#   _target_: torch.optim.lr_scheduler.${scheduler.name}
#   T_max: ${n_epochs}
#   eta_min: ${scheduler.eta_min}

# Experiment tracking config (optional) ----------------------------------------
# tracker: null # set to null to disable
tracker:
  _target_: aim.Run
  repo: .
  experiment: ${hydra:job.config_name}
