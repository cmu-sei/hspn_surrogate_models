#
# HyperSPIN code - hspn_surrogate_models
# 
# Copyright 2025 Carnegie Mellon University.
# 
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
# 
# Licensed under a MIT (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
# 
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see Copyright notice for non-US Government use and distribution.
# 
# This Software includes and/or makes use of Third-Party Software each subject to its own license.
# 
# DM25-0396
#

# src/hspn/conf/train_hpo_optuna.yaml
# A hyperparameter optimization sweep using the optuna sweeper with the grid sampler.
# Inherits from train.yaml, we just specify what we want to sweep over (and override the run mode for convenience)
# Uses the default launcher that simply launches trials sequentially. Of course can override at CLI, e.g. "hydra/launcher=joblib"
#
# Usage: python -m hspn.train --config-name=train_hpo_optuna
#
# Documentation:
#  - Sweeper configuration options: https://hydra.cc/docs/plugins/optuna_sweeper/
#  - Optuna samplers:https://optuna.readthedocs.io/en/stable/reference/samplers/index.html 

defaults:
  - train
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - override hydra/hydra_logging: default
  - override hydra/job_logging: default
  - _self_

hydra:
  mode: MULTIRUN # equivalent to -m or --multirun on the command line. (Without specifying this at some point the sweep is not performed)
  sweeper:
   direction: minimize
   study_name: ${oc.env:STUDY_NAME,hspn}
   storage: ${oc.env:OPTUNA_STORAGE_URL,null}
   n_trials: ${oc.env:N_TRIALS,100} # max number of trials, the sweeper may find it needs fewer trials
   n_jobs: ${oc.env:N_WORKERS,1}
   params:
     optimizer.lr: choice(1e-5, 3e-5)
     model.trunk_config.width: choice(50, 100)
     model.trunk_config.depth: choice(4, 6)
     model.latent_dim: choice(10, 25)
n_epochs: 300
# dataloader:
#   dataset:
#     trunk_batch_size: 10_000_000
# val_dataloader:
#   dataset:
#
#     trunk_batch_size: 10_000_000
# # 1200e 2hr 8.5M bs w/ largest net trunk_depth=8 trunk_width=100 latent=50 (other models around 1300e)
# model:
#   latent_dim: 50
# n_epochs: 1400
# dataloader:
#   dataset:
#     trunk_batch_size: 8_500_000
# val_dataloader:
#   dataset:
#     trunk_batch_size: 8_500_000
