# src/hspn/conf/train_hpo_optuna.yaml
# A hyperparameter optimization sweep using the optuna sweeper with the grid sampler.
# Inherits from train.yaml, we just specify what we want to sweep over (and override the run mode for convenience)
# Uses the default launcher that simply launches trials sequentially. Of course can override at CLI, e.g. "hydra/launcher=joblib"
#
# Usage: python -m hspn.train --config-name=train_hpo_optuna
#
# Documentation:
#  - Sweeper configuration options: https://hydra.cc/docs/plugins/optuna_sweeper/
#  - Optuna samplers:https://optuna.readthedocs.io/en/stable/reference/samplers/index.html 

defaults:
  - train
  - _self_
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: grid

hydra:
  mode: MULTIRUN # equivalent to -m or --multirun on the command line. (Without specifying this at some point the sweep is not performed)
  sweeper:
   direction: minimize
   study_name: hspn
   storage: null
   n_trials: 10
   n_jobs: 1
   params:
     optimizer_factory._target_: choice(torch.optim.SGD ,torch.optim.Adam)
     optimizer_factory.lr: range(0.0001, 0.0005, step=0.0001)

