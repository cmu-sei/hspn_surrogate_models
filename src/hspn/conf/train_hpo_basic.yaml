# src/hspn/conf/train_hpo_basic.yaml
# A hyperparameter optimization sweep using the default sweeper that runs all combinations of params.
# Inherits from train.yaml, we just specify what we want to sweep over
# Uses the default launcher that simply launches trials sequentially. Of course can override at CLI, e.g. "hydra/launcher=joblib"
# Usage: python -m hspn.train --config-name=train_hpo_basic
defaults:
  - train
  - _self_

hydra:
  mode: MULTIRUN # equivalent to -m or --multirun on the command line. (Without specifying this at some point the sweep is not performed)
  sweeper:
    params:
      optimizer_factory._target_: torch.optim.Adam
      # optimizer_factory.lr: choice(1e-5, 3e-5, 1e-4, 3e-4)
      # model.trunk_config.width: choice(50, 100)
      model.trunk_config.depth: choice(4, 5, 6)
      model.latent_dim: choice(10, 25)
