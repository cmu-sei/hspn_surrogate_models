defaults:
  - train_spec
  - _self_
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: grid

hydra:
  mode: MULTIRUN
  output_subdir: hydra
  sweeper:
   direction: minimize
   study_name: hspn
   storage: null
   n_trials: 10
   n_jobs: 1
   params:
     optimizer_factory._target_: choice(torch.optim.SGD ,torch.optim.Adam)
     optimizer_factory.lr: range(0.001, 0.005, step=0.002)

# Basic training parameters
seed: 42
n_epochs: 1
comm_backend: nccl
log_interval: 50 # number of batches

# Model configuration
model:
  _target_: hspn.model.DeepOperatorNet
  branch_dim: 1
  trunk_dim: 3
  branch_config:
    width: 100
    depth: 4
    activation:
      _target_: torch.nn.ELU
  trunk_config:
    width: 100
    depth: 5
    activation:
      _target_: torch.nn.ReLU
  latent_dim: 25
  einsum_pattern: ij,kj->ik

# Data configuration
dataloader:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: hspn.dataset.H5Dataset
    file_path: ${extra.data_dir}/don_dataset.h5
    # TODO: support uneven batch sizes? ask jasmine
    branch_batch_size: 100000
    trunk_batch_size: 100000
  batch_size: null  # Do not change this
  shuffle: false # Do not change this
  num_workers: 0
  pin_memory: true
  prefetch_factor: null

# Optimizer configuration
optimizer_factory:
  _target_: torch.optim.SGD
  _partial_: true
  lr: 0.0005

# Scheduler configuration
scheduler_factory: null

tracker_config: ${extra.aim_config}

extra:
  data_dir: ${oc.env:DATA_DIR,./data}
  tb_config:
    log_dir: ${checkpoint_dir}
    backend: tensorboard
    experiment_name: ${now:%Y-%m-%d_%H-%M-%S}_${hydra:job.name}
  aim_config:
    log_dir: .
    backend: aim
    experiment_name: ${now:%Y-%m-%d_%H-%M-%S}_${hydra:job.name}

