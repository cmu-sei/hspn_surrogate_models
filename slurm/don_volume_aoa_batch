#! /bin/bash
#SBATCH --account=ARLAP44862YFR
#SBATCH --job-name=hyperspin_test
#SBATCH --partition=general

#SBATCH --time=0:30:00

##SBATCH --qos=debug
#SBATCH --qos=frontier

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --exclusive
#SBATCH --mem=12G

#######################################################
## AI/ML nodes are GPU nodes with specialized hardware 
## and software to support machine-learning tasks. 
## To request an AI/ML node, use the directive:
##SBATCH --constraint=mla
##SBATCH --constraint=viz

#################################################################
## Request GPU nodes - select one of the options below
#SBATCH --gres=gpu:a100:4  # Same as mla
##SBATCH --gres=gpu:a40:2   # Same as viz
##SBATCH --gres=gpu:2
##SBATCH --gres=gpu:4



#################################################################
## Job information set by Baseline Configuration variables
#################################################################
echo ----------------------------------------------------------
echo "Type of node                    " $BC_NODE_TYPE
echo "CPU cores per node              " $BC_CORES_PER_NODE
echo "CPU cores per standard node     " $BC_STANDARD_NODE_CORES
echo "CPU cores per accelerator node  " $BC_ACCELERATOR_NODE_CORES
echo "CPU cores per big memory node   " $BC_BIGMEM_NODE_CORES
echo "Hostname                        " $BC_HOST
echo "Maxumum memory per nodes        " $BC_MEM_PER_NODE
echo "Number of tasks allocated       " $BC_MPI_TASKS_ALLOC
echo "Number of nodes allocated       " $BC_NODE_ALLOC
echo ----------------------------------------------------------

##############################################################
## Output some useful job information.  
##############################################################
echo "-------------------------------------------------------"
echo "Project ID                      " $SLURM_JOB_ACCOUNT
echo "Job submission directory        " $SLURM_SUBMIT_DIR
echo "Submit host                     " $SLURM_SUBMIT_HOST
echo "Job name                        " $SLURM_JOB_NAME
echo "Job identifier (SLURM_JOB_ID)   " $SLURM_JOB_ID
echo "Job identifier (SLURM_JOBID)    " $SLURM_JOBID
echo "Working directory               " $WORKDIR
echo "Job partition                   " $SLURM_JOB_PARTITION
echo "Job queue (QOS)                 " $SLURM_JOB_QOS
echo "Job number of nodes             " $SLURM_JOB_NUM_NODES
echo "Job node list                   " $SLURM_JOB_NODELIST
echo "Number of nodes                 " $SLURM_NNODES
echo "Number of tasks                 " $SLURM_NTASKS
echo "Node list                       " $SLURM_NODELIST
echo "-------------------------------------------------------"
echo

module purge
module load slurm
module load penguin/openmpi/4.1.4/gcc
module load nvidia/openmpi/gnu/4.1
module load cuda/cuda-12.4

CONTAINER=/p/app/containers/tensorflow/tensorflow-22.03-tf2-py3.sif
SCRIPT=/p/home/jyuko/projects/hspn_surrogate_models/scripts/run_train_don.sh


# Check GPU availability
srun nvidia-smi

# Run The Job Through Singularity
echo "mpirun -np 2 singularity exec  -B /p/home/jyuko,/p/work1/projects --nv ${CONTAINER} ${SCRIPT}"
mpirun -np 2 singularity exec  -B /p/home/jyuko,/p/work1/projects --nv $CONTAINER $SCRIPT
#srun --partition=general --account=ARLAP44862YFR --qos=frontier --time=2:00:00 --constraint=mla --exclusive --gres=gpu:2 $CONTAINER $SCRIPT